{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_read():\n",
    "    \n",
    "    \"\"\"function to read data into pandas dataframe,\n",
    "    further convert sentiment column into numerical values\"\"\"\n",
    "    \n",
    "    global df\n",
    "    \n",
    "    df = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "    df = df.head(int(len(df)/5))\n",
    "    df['sentiment']= df['sentiment'].apply(lambda x : 1 if x=='positive' else 0)\n",
    "    \n",
    "def html_tag_remover():\n",
    "    \n",
    "    \"\"\"function to remove html tags using regex, \n",
    "    and store a copy of dataframe in variable\"\"\"\n",
    "    \n",
    "    global df_removed_tag\n",
    "    \n",
    "    df['review'] = df['review'].str.replace(r'<[^<>]*>', '', regex = True)\n",
    "    df_removed_tag = df\n",
    "    \n",
    "def url_remover():\n",
    "    \n",
    "    \"\"\"function to remove url using regex, \n",
    "    and store a copy of dataframe in variable\"\"\"\n",
    "    \n",
    "    global df_removed_url\n",
    "    \n",
    "    df['review'] = df['review'].str.replace(r'https ? ://\\s+|www\\.\\s+', '', regex = True)\n",
    "    df_removed_tag = df\n",
    "    \n",
    "def lowercase():\n",
    "    \n",
    "    \"\"\"function to convert review into lowercase, \n",
    "    and store a copy of dataframe in variable\"\"\"\n",
    "    \n",
    "    global df_lower\n",
    "    \n",
    "    df['review'] = df['review'].str.lower()\n",
    "    df_lower = df\n",
    "    \n",
    "def punctuation_remover():\n",
    "    \n",
    "    \"\"\"function to remove punctuation using regex, \n",
    "    and store a copy of dataframe in variable\"\"\"\n",
    "    \n",
    "    global df_punc_removed\n",
    "    \n",
    "    df['review'] = df['review'].str.replace('[{}]'.format(string.punctuation), '', regex = True)\n",
    "    df_punc_removed = df\n",
    "    \n",
    "def stopword_remover():\n",
    "    \n",
    "    \"\"\"function to remove stopwords, \n",
    "    and store a copy of dataframe in variable\"\"\"\n",
    "    \n",
    "    global df_stopword_removed\n",
    "    \n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))\n",
    "    df_stopword_removed = df\n",
    "    \n",
    "def lemmatize_text():\n",
    "    \n",
    "    \"\"\"function to lemmatize reviews, \n",
    "    and store a copy of dataframe in variable\"\"\"\n",
    "    \n",
    "    global df_lemmatized\n",
    "    \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)]))\n",
    "    df_lemmatized =  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess():\n",
    "    \n",
    "    data_read()\n",
    "    html_tag_remover()\n",
    "    url_remover()\n",
    "    lowercase()\n",
    "    punctuation_remover()\n",
    "    stopword_remover()\n",
    "    lemmatize_text()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocess()\n",
    "\n",
    "\"\"\"split the dataset\n",
    "75% for training\n",
    "25% for testing\"\"\"\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.25, random_state = 42)\n",
    "\n",
    "X_train, y_train = train['review'], train['sentiment']\n",
    "X_test, y_test = test['review'], test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Natural Language Processing technique of text modeling known as Bag of Words model. \n",
    "> Whenever we apply any algorithm in NLP, it works on numbers. \n",
    "> We cannot directly feed our text into that algorithm. \n",
    "> Hence, Bag of Words model is used to preprocess the text by converting it into a bag of words, \n",
    "> which keeps a count of the total occurrences of most frequently used words\"\"\"\n",
    "\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "cnt_vec = CountVectorizer(ngram_range = (1, 3), binary = True)\n",
    "x_train_vector = cnt_vec.fit_transform(X_train)\n",
    "x_test_vector = cnt_vec.transform(X_test)\n",
    "\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train_vector_2 = tfidf.fit_transform(X_train)\n",
    "x_test_vector_2 = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(y_pred):\n",
    "    \n",
    "    \"\"\"This function will show results\"\"\"\n",
    "    \n",
    "    print(\"Classification Report: \\n\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix: \\n\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy: \\n\\n\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_NaiveB(train_data_vector, test_data_vector):\n",
    "    \n",
    "    \"\"\"function to produce results\n",
    "    used Multinomial naive Bayes Model\"\"\"\n",
    "    \n",
    "    multi_clf = MultinomialNB()\n",
    "    multi_clf.fit(train_data_vector, y_train.values)\n",
    "\n",
    "    predict_NB = multi_clf.predict(test_data_vector)\n",
    "\n",
    "    return result(predict_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86      1220\n",
      "           1       0.90      0.82      0.86      1280\n",
      "\n",
      "    accuracy                           0.86      2500\n",
      "   macro avg       0.86      0.86      0.86      2500\n",
      "weighted avg       0.86      0.86      0.86      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[1103  117]\n",
      " [ 229 1051]]\n",
      "Accuracy: \n",
      "\n",
      " 0.8616\n"
     ]
    }
   ],
   "source": [
    "multinomial_NaiveB(x_train_vector, x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85      1220\n",
      "           1       0.89      0.81      0.85      1280\n",
      "\n",
      "    accuracy                           0.85      2500\n",
      "   macro avg       0.85      0.85      0.85      2500\n",
      "weighted avg       0.85      0.85      0.85      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[1087  133]\n",
      " [ 239 1041]]\n",
      "Accuracy: \n",
      "\n",
      " 0.8512\n"
     ]
    }
   ],
   "source": [
    "multinomial_NaiveB(x_train_vector_2, x_test_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_SVC(train_data_vector, test_data_vector):\n",
    "    \n",
    "    \"\"\"function to produce results\n",
    "    used Linear SVC Kernel Model\"\"\"\n",
    "\n",
    "    linear_svc = LinearSVC(C = 0.5, random_state = 42)\n",
    "    linear_svc.fit(train_data_vector, y_train.values)\n",
    "\n",
    "    predict_Lin_Svc = linear_svc.predict(test_data_vector)\n",
    "\n",
    "    return result(predict_Lin_Svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      1220\n",
      "           1       0.87      0.89      0.88      1280\n",
      "\n",
      "    accuracy                           0.87      2500\n",
      "   macro avg       0.87      0.87      0.87      2500\n",
      "weighted avg       0.87      0.87      0.87      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[1049  171]\n",
      " [ 143 1137]]\n",
      "Accuracy: \n",
      "\n",
      " 0.8744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anish/Documents/NLP_proj/nlp/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Linear_SVC(x_train_vector, x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      1220\n",
      "           1       0.87      0.89      0.88      1280\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.88      0.87      0.88      2500\n",
      "weighted avg       0.88      0.88      0.88      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[1045  175]\n",
      " [ 137 1143]]\n",
      "Accuracy: \n",
      "\n",
      " 0.8752\n"
     ]
    }
   ],
   "source": [
    "Linear_SVC(x_train_vector_2, x_test_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest(train_data_vector, test_data_vector):\n",
    "    \n",
    "    \"\"\"function to produce results\n",
    "    used Random Forest Model\"\"\"\n",
    "\n",
    "    forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    forest.fit(train_data_vector, y_train.values)\n",
    "\n",
    "    forest_predict = forest.predict(test_data_vector)\n",
    "\n",
    "    return result(forest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77      1220\n",
      "           1       0.79      0.74      0.77      1280\n",
      "\n",
      "    accuracy                           0.77      2500\n",
      "   macro avg       0.77      0.77      0.77      2500\n",
      "weighted avg       0.77      0.77      0.77      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[974 246]\n",
      " [329 951]]\n",
      "Accuracy: \n",
      "\n",
      " 0.77\n"
     ]
    }
   ],
   "source": [
    "Random_Forest(x_train_vector, x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.76      1220\n",
      "           1       0.80      0.69      0.74      1280\n",
      "\n",
      "    accuracy                           0.75      2500\n",
      "   macro avg       0.76      0.75      0.75      2500\n",
      "weighted avg       0.76      0.75      0.75      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[1002  218]\n",
      " [ 399  881]]\n",
      "Accuracy: \n",
      "\n",
      " 0.7532\n"
     ]
    }
   ],
   "source": [
    "Random_Forest(x_train_vector_2, x_test_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_regression(train_data_vector, test_data_vector):\n",
    "    \n",
    "    \"\"\"function to produce results\n",
    "    used Logistic Regression Model\"\"\"\n",
    "\n",
    "    l_regression = LogisticRegression(random_state = 0)\n",
    "    l_regression.fit(train_data_vector, y_train.values)\n",
    "\n",
    "    regression_predict = l_regression.predict(test_data_vector)\n",
    "\n",
    "    return result(regression_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      1220\n",
      "           1       0.87      0.89      0.88      1280\n",
      "\n",
      "    accuracy                           0.87      2500\n",
      "   macro avg       0.87      0.87      0.87      2500\n",
      "weighted avg       0.87      0.87      0.87      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[1047  173]\n",
      " [ 144 1136]]\n",
      "Accuracy: \n",
      "\n",
      " 0.8732\n"
     ]
    }
   ],
   "source": [
    "Logistic_regression(x_train_vector, x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      1220\n",
      "           1       0.86      0.88      0.87      1280\n",
      "\n",
      "    accuracy                           0.87      2500\n",
      "   macro avg       0.87      0.87      0.87      2500\n",
      "weighted avg       0.87      0.87      0.87      2500\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[1038  182]\n",
      " [ 149 1131]]\n",
      "Accuracy: \n",
      "\n",
      " 0.8676\n"
     ]
    }
   ],
   "source": [
    "Logistic_regression(x_train_vector_2, x_test_vector_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
