{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('omw-1.4')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-20T16:34:35.186151Z","iopub.execute_input":"2022-12-20T16:34:35.186698Z","iopub.status.idle":"2022-12-20T16:34:37.550858Z","shell.execute_reply.started":"2022-12-20T16:34:35.186589Z","shell.execute_reply":"2022-12-20T16:34:37.549525Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preprocessing\n\n> **About Dataset**\n\n> IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n\n> This is a dataset for binary sentiment classification\n\n> a set of 25,000 highly polar movie reviews for training and 25,000 for testing is provided\n\n> For more dataset information, please go through the following link :\nhttp://ai.stanford.edu/~amaas/data/sentiment/","metadata":{}},{"cell_type":"code","source":"def data_read():\n    \n    \"\"\"function to read data into pandas dataframe,\n    further convert sentiment column into numerical values\"\"\"\n    \n    global df\n    \n    df = df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n    df['sentiment']= df['sentiment'].apply(lambda x : 1 if x=='positive' else 0)\n    \ndef html_tag_remover():\n    \n    \"\"\"function to remove html tags using regex, \n    and store a copy of dataframe in variable\"\"\"\n    \n    global df_removed_tag\n    \n    df['review'] = df['review'].str.replace(r'<[^<>]*>', '', regex = True)\n    df_removed_tag = df\n    \ndef url_remover():\n    \n    \"\"\"function to remove url using regex, \n    and store a copy of dataframe in variable\"\"\"\n    \n    global df_removed_url\n    \n    df['review'] = df['review'].str.replace(r'https ? ://\\s+|www\\.\\s+', '', regex = True)\n    df_removed_tag = df\n    \ndef lowercase():\n    \n    \"\"\"function to convert review into lowercase, \n    and store a copy of dataframe in variable\"\"\"\n    \n    global df_lower\n    \n    df['review'] = df['review'].str.lower()\n    df_lower = df\n    \ndef punctuation_remover():\n    \n    \"\"\"function to remove punctuation using regex, \n    and store a copy of dataframe in variable\"\"\"\n    \n    global df_punc_removed\n    \n    df['review'] = df['review'].str.replace('[{}]'.format(string.punctuation), '', regex = True)\n    df_punc_removed = df\n    \ndef stopword_remover():\n    \n    \"\"\"function to remove stopwords, \n    and store a copy of dataframe in variable\"\"\"\n    \n    global df_stopword_removed\n    \n    df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))\n    df_stopword_removed = df\n    \ndef lemmatize_text():\n    \n    \"\"\"function to lemmatize reviews, \n    and store a copy of dataframe in variable\"\"\"\n    \n    global df_lemmatized\n    \n    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    \n    df['review'] = df['review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)]))\n    df_lemmatized =  df","metadata":{"execution":{"iopub.status.busy":"2022-12-20T16:34:39.188815Z","iopub.execute_input":"2022-12-20T16:34:39.189986Z","iopub.status.idle":"2022-12-20T16:34:42.466602Z","shell.execute_reply.started":"2022-12-20T16:34:39.189930Z","shell.execute_reply":"2022-12-20T16:34:42.465020Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def data_preprocess():\n    \n    data_read()\n    html_tag_remover()\n    url_remover()\n    lowercase()\n    punctuation_remover()\n    stopword_remover()\n    lemmatize_text()\n    \n    return","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# classifier","metadata":{}},{"cell_type":"code","source":"data_preprocess()\n\n\"\"\"split the dataset\n75% for training\n25% for testing\"\"\"\n\ntrain, test = train_test_split(df, test_size = 0.25, random_state = 42)\n\nX_train, y_train = train['review'], train['sentiment']\nX_test, y_test = test['review'], test['sentiment']","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:33:38.200588Z","iopub.execute_input":"2022-12-20T17:33:38.201273Z","iopub.status.idle":"2022-12-20T17:33:38.219696Z","shell.execute_reply.started":"2022-12-20T17:33:38.201232Z","shell.execute_reply":"2022-12-20T17:33:38.218671Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\"\"\"Natural Language Processing technique of text modeling known as Bag of Words model. \n> Whenever we apply any algorithm in NLP, it works on numbers. \n> We cannot directly feed our text into that algorithm. \n> Hence, Bag of Words model is used to preprocess the text by converting it into a bag of words, \n> which keeps a count of the total occurrences of most frequently used words\"\"\"\n\ntfidf = TfidfVectorizer()\nx_train_vector = tfidf.fit_transform(X_train)\nx_train_vector.toarray()\n\nx_test_vector = tfidf.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:36:00.757156Z","iopub.execute_input":"2022-12-20T17:36:00.757717Z","iopub.status.idle":"2022-12-20T17:36:26.563404Z","shell.execute_reply.started":"2022-12-20T17:36:00.757677Z","shell.execute_reply":"2022-12-20T17:36:26.562149Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"multi_clf = MultinomialNB()\nmulti_clf.fit(x_train_vector, y_train.values)\n\npredict_NB = multi_clf.predict(x_test_vector)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:37:36.489496Z","iopub.execute_input":"2022-12-20T17:37:36.490039Z","iopub.status.idle":"2022-12-20T17:37:36.588327Z","shell.execute_reply.started":"2022-12-20T17:37:36.489996Z","shell.execute_reply":"2022-12-20T17:37:36.586465Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report: \\n\\n\", classification_report(y_test, predict_NB))\nprint(\"Confusion Matrix: \\n\\n\", confusion_matrix(y_test, predict_NB))\nprint(\"Accuracy: \\n\\n\", accuracy_score(y_test, predict_NB))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:41:08.997626Z","iopub.execute_input":"2022-12-20T17:41:08.998154Z","iopub.status.idle":"2022-12-20T17:41:09.042576Z","shell.execute_reply.started":"2022-12-20T17:41:08.998114Z","shell.execute_reply":"2022-12-20T17:41:09.040753Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Classification Report: \n\n               precision    recall  f1-score   support\n\n           0       0.85      0.89      0.87      6157\n           1       0.88      0.84      0.86      6343\n\n    accuracy                           0.87     12500\n   macro avg       0.87      0.87      0.87     12500\nweighted avg       0.87      0.87      0.87     12500\n\nConfusion Matrix: \n\n [[5459  698]\n [ 989 5354]]\nAccuracy: \n\n 0.86504\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# result\n\n* creating a dataframe from test data \n* review column contains reviews\n* sentiment column contains actual sentiments\n* predicted_sentiment column contains predicted sentiments","metadata":{}},{"cell_type":"code","source":"dataset_predict = X_test.copy()\ndataset_predict = pd.DataFrame(dataset_predict)\ndataset_predict.columns = ['review']\ndataset_predict = dataset_predict.reset_index()\ndataset_predict = dataset_predict.drop(['index'], axis = 1)\n\ntest_actual_label = (y_test.values).copy()\ntest_actual_label = pd.DataFrame(test_actual_label)\ntest_actual_label.columns = ['sentiment']\ntest_actual_label['sentiment'] = test_actual_label['sentiment'].replace({1: 'positive', 0: 'negative'})\n\ntest_predicted_label = predict_NB.copy()\ntest_predicted_label = pd.DataFrame(test_predicted_label)\ntest_predicted_label.columns = ['predicted_sentiment']\ntest_predicted_label['predicted_sentiment'] = test_predicted_label['predicted_sentiment'].replace({1: 'positive', 0: 'negative'})\n\ntest_result = pd.concat([dataset_predict, test_actual_label, test_predicted_label], axis=1)\ntest_result","metadata":{"execution":{"iopub.status.busy":"2022-12-20T18:31:57.027049Z","iopub.execute_input":"2022-12-20T18:31:57.027522Z","iopub.status.idle":"2022-12-20T18:31:57.067714Z","shell.execute_reply.started":"2022-12-20T18:31:57.027487Z","shell.execute_reply":"2022-12-20T18:31:57.066252Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment  \\\n0      really liked summerslam due look arena curtain...  positive   \n1      many television show appeal quite many differe...  positive   \n2      film quickly get major chase scene ever increa...  negative   \n3      jane austen would definitely approve onegwynet...  positive   \n4      expectation somewhat high went see movie thoug...  negative   \n...                                                  ...       ...   \n12495  first separate story film story second continu...  negative   \n12496  obvious flawhorrible horrible script movie pot...  negative   \n12497  brilliance movie even competent dentist pretty...  positive   \n12498  yaitate japan really fun show really like show...  positive   \n12499  wonderfully funny aweinspiring feature pioneer...  positive   \n\n      predicted_sentiment  \n0                positive  \n1                positive  \n2                negative  \n3                positive  \n4                negative  \n...                   ...  \n12495            negative  \n12496            negative  \n12497            negative  \n12498            positive  \n12499            positive  \n\n[12500 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>predicted_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>really liked summerslam due look arena curtain...</td>\n      <td>positive</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>many television show appeal quite many differe...</td>\n      <td>positive</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>film quickly get major chase scene ever increa...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jane austen would definitely approve onegwynet...</td>\n      <td>positive</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>expectation somewhat high went see movie thoug...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12495</th>\n      <td>first separate story film story second continu...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>12496</th>\n      <td>obvious flawhorrible horrible script movie pot...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>12497</th>\n      <td>brilliance movie even competent dentist pretty...</td>\n      <td>positive</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>12498</th>\n      <td>yaitate japan really fun show really like show...</td>\n      <td>positive</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>12499</th>\n      <td>wonderfully funny aweinspiring feature pioneer...</td>\n      <td>positive</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>12500 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]}]}